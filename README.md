Back in 2017, when the Google published "Attention is All You Need" for NLP problems, people might not have thought about implementing the "attention" on vision data. Almost 4 years after this, Google came up with a way of implementing the "attention mechanism" for computer vision in the paper titled "An Image Is Worth 16X16 Words:Transformers For Image Recognition At Scale".
<br>
I first tried out the implementation of Vision Transformer by lucidrains at https://lnkd.in/ehh9b98r.
<br>
Then I decided to give a shot to actually implement the vision transformer using pytorch just as described in the paper.
